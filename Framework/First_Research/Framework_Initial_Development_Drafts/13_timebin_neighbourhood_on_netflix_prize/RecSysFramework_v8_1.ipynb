{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Based on Timebin Neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes:\n",
    "\n",
    "* [Dataset](#Dataset)\n",
    " * [Netflix Dataset](#NetflixDataset)\n",
    " * [Movielens Dataset](#MovielensDataset)\n",
    " \n",
    " \n",
    "* [Time Constraint](#TimeConstraint)\n",
    "* [Accuracy](#Accuracy)\n",
    "\n",
    "\n",
    "* [TrainsetUser](#TrainsetUser)\n",
    "* [TrainsetMovie](#TrainsetMovie)\n",
    "\n",
    "\n",
    "* [Pearson](#Pearson)\n",
    "\n",
    "\n",
    "* [Trainset](#Trainset)\n",
    "\n",
    "\n",
    "* [TimebinSimilarity](#TimebinSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from random import sample\n",
    "from collections import deque  # To shift lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is the bases class of all datasets. Defines the basic load method which has to be defined by every subclass of the dataset.\n",
    "* Currently Supported Datasets:\n",
    "  * Netflix Dataset\n",
    "  * Movielens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(ABC):\n",
    "    \"\"\"\n",
    "    Dataset class and its subclasses provides utilities in order to import datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load():\n",
    "        \"\"\" Every subclass must provide static load method\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetflixDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Netflix Dataset](https://www.kaggle.com/netflix-inc/netflix-prize-data) consists of over 100million ratings with 470k users.\n",
    "\n",
    "But the problem is, the test machine that we use can not handle more than half a million ratings. That is why we had to truncate most of the data out of the dataset.\n",
    "* The way we truncate the dataset is as follows:\n",
    "  1. We only take 1 out of 4 combined data which all together constructs the netflix prize dataset.\n",
    "  2. Then we drop the duplicate users where they have mean rating in common or number of ratings given in common.\n",
    "  3. At the end of the all these, we get a dataset which has 4499 movies, 480k ratings data with 850 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetflixDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ratings_col_names=('user_id', 'rating', 'timestamp'),\n",
    "                       ratings_path=r'C:\\Users\\Yukawa\\datasets\\netflix\\combined_data_1.txt',\n",
    "                       movies_col_names=('item_id', 'year', 'title'),\n",
    "                       movies_path=r'C:\\Users\\Yukawa\\datasets\\netflix\\movie_titles.csv',\n",
    "                       is_ratings_cached=True, is_movies_cached=True, \n",
    "                       is_movie_ratings_cached=True):\n",
    "        Dataset.__init__(self)\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.ratings = NetflixDataset.load_ratings(ratings_path, \n",
    "                                                   ratings_col_names) if self.is_ratings_cached else None\n",
    "        self.movies = NetflixDataset.load_movies(movies_path,\n",
    "                                                   movies_col_names) if self.is_movies_cached else None\n",
    "        self.is_movie_ratings_cached = is_movie_ratings_cached\n",
    "        self.movie_ratings = NetflixDataset.create_movie_ratings(self.ratings, \n",
    "                                                                 self.movies) if self.is_movie_ratings_cached else None\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_movies(movies_path,\n",
    "                    movies_col_names=('item_id', 'year', 'title')):\n",
    "        \n",
    "        \n",
    "        movies = pd.read_csv(movies_path, encoding='ISO-8859-1', header=None, names=movies_col_names).set_index('item_id')\n",
    "        movies['year'].replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "        movies['year'] = movies['year'].astype(int)\n",
    "        movies = movies.reindex(columns=['title', 'year'])\n",
    "        \n",
    "        # From the netflix prize dataset, I will only be using the first part which contains 4449 unique movies. \n",
    "        # That is why I will be truncating the other movies but if you load all the netflix data, remove this line.\n",
    "        movies = movies[:4499]    # Keep only the first 4499 movies of the dataset\n",
    "        \n",
    "        return movies\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_ratings(ratings_path,\n",
    "                     ratings_col_names=('user_id', 'rating', 'timestamp')):    \n",
    "        if not os.path.isfile(ratings_path) or not ratings_col_names:\n",
    "            return None\n",
    "        \n",
    "        ratings_raw = pd.read_csv(ratings_path, header=None, names=['user_id', 'rating', 'timestamp'], usecols=[0, 1, 2])\n",
    "        ratings_raw['rating'] = ratings_raw['rating'].astype(float)\n",
    "        # Find empty rows to slice dataframe for each movie\n",
    "        temp_movies = ratings_raw[ratings_raw['rating'].isna()]['user_id'].reset_index()\n",
    "        movie_indexes = [[index, int(movie[:-1])] for index, movie in temp_movies.values]\n",
    "\n",
    "        # Shift the movie_indexes by one to get start and endpoints of all movies\n",
    "        shifted_movie_indexes = deque(movie_indexes)\n",
    "        shifted_movie_indexes.rotate(-1)\n",
    "\n",
    "        # Gather all dataframes\n",
    "        user_data = []\n",
    "        for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indexes, shifted_movie_indexes):\n",
    "\n",
    "            # if it is the last movie in the file\n",
    "            if df_id_1 < df_id_2:\n",
    "                temp_df = ratings_raw[(df_id_1+1):(df_id_2-1)].copy()\n",
    "            else:\n",
    "                temp_df = ratings_raw[df_id_1+1:].copy()\n",
    "\n",
    "            # Create movie id column\n",
    "            temp_df['item_id'] = movie_id\n",
    "\n",
    "            # Append dataframe to list\n",
    "\n",
    "            user_data.append(temp_df)\n",
    "\n",
    "        # Combile all  dataframes\n",
    "        ratings = pd.concat(user_data)\n",
    "        del user_data, ratings_raw, temp_movies, temp_df, shifted_movie_indexes, movie_indexes, df_id_1, df_id_2, movie_id, next_movie_id\n",
    "        \n",
    "        # Convert the column order to the same order as the MovieLens dataset for ease of use\n",
    "        ratings = ratings.reindex(columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "        \n",
    "        # Convert the string timestamps into datetime type\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], infer_datetime_format=True)\n",
    "        \n",
    "        # Convert the string user ids into int\n",
    "        ratings['user_id'] = ratings['user_id'].astype(int)\n",
    "        \n",
    "        #### !!! Drop These two lines if No Memory and Time Constraint are set.\n",
    "        netflix_users = NetflixDataset.get_filtered_netflix_users(ratings)\n",
    "        ratings = ratings.loc[ (ratings['user_id'].isin(netflix_users)) ]\n",
    "        \n",
    "        return ratings\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_filtered_netflix_users(ratings):\n",
    "        data = ratings\n",
    "        active_users = pd.DataFrame(data.groupby('user_id')['rating'].mean())\n",
    "        active_users['No_of_ratings'] = pd.DataFrame(data.groupby('user_id')['rating'].count())\n",
    "        active_users.sort_values(by=['No_of_ratings'], ascending=False, inplace=True)\n",
    "        active_users.columns = ['mean_rating', 'No_of_ratings']\n",
    "        return active_users.loc[active_users['No_of_ratings'] > 40].drop_duplicates('mean_rating').drop_duplicates('No_of_ratings').index.values\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_movie_ratings(ratings, movies):\n",
    "        return pd.merge(ratings, movies, on='item_id')\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load(ratings_col_names=('user_id', 'rating', 'timestamp'),\n",
    "             ratings_path=r'C:\\Users\\Yukawa\\datasets\\netflix\\combined_data_1.txt',\n",
    "             movies_col_names=('item_id', 'year', 'title'),\n",
    "             movies_path=r'C:\\Users\\Yukawa\\datasets\\netflix\\movie_titles.csv'):\n",
    "        \n",
    "        # Load movies\n",
    "        movies = NetflixDataset.load_movies(movies_path=movies_path, movies_col_names=movies_col_names)\n",
    "        \n",
    "        # Load Ratings\n",
    "        ratings = NetflixDataset.load_ratings(ratings_path=ratings_path, ratings_col_names=ratings_col_names)\n",
    "        \n",
    "        # Merge the ratings and movies\n",
    "        movie_ratings = NetflixDataset.create_movie_ratings(ratings, movies)\n",
    "        \n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovielensDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Movie Lens Latest Small Dataset](https://grouplens.org/datasets/movielens/) has 100,000 ratings and 9,000 movies with 600 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "                 ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "                 movies_col_names=('item_id', 'title', 'genres'),\n",
    "                 movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv',\n",
    "                 is_ratings_cached=True,\n",
    "                 is_movies_cached=True, \n",
    "                 is_movie_ratings_cached=True):\n",
    "        Dataset.__init__(self)\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.ratings = MovieLensDataset.load_ratings(ratings_path,\n",
    "                                                     ratings_col_names) if self.is_ratings_cached else None\n",
    "        self.movies = MovieLensDataset.load_movies(movies_path,\n",
    "                                                   movies_col_names) if self.is_movies_cached else None\n",
    "        self.is_movie_ratings_cached = is_movie_ratings_cached\n",
    "        self.movie_ratings = MovieLensDataset.create_movie_ratings(self.ratings, \n",
    "                                                                 self.movies) if self.is_movie_ratings_cached else None\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_movies(movies_path,\n",
    "                    movies_col_names=('item_id', 'title', 'genres')):\n",
    "        if not os.path.isfile(movies_path) or not movies_col_names:\n",
    "            return None\n",
    "\n",
    "        # read movies\n",
    "        movies = pd.read_csv(movies_path, sep=',', header=1, names=movies_col_names)\n",
    "\n",
    "        # Extract Movie Year\n",
    "        movies['year'] = movies.title.str.extract(\"\\((\\d{4})\\)\", expand=True)\n",
    "        movies.year = pd.to_datetime(movies.year, format='%Y')\n",
    "        movies.year = movies.year.dt.year  # As there are some NaN years, resulting type will be float (decimals)\n",
    "\n",
    "        # Remove year part from the title\n",
    "        movies.title = movies.title.str[:-7]\n",
    "\n",
    "        return movies\n",
    "\n",
    "    @staticmethod\n",
    "    def load_ratings(ratings_path,\n",
    "                     ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp')):\n",
    "        if not os.path.isfile(ratings_path) or not ratings_col_names:\n",
    "            return None\n",
    "\n",
    "        # read ratings\n",
    "        ratings = pd.read_csv(ratings_path, sep=',', header=1, names=ratings_col_names)\n",
    "\n",
    "        # Convert timestamp into readable format\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s', origin='unix')\n",
    "\n",
    "        return ratings\n",
    "\n",
    "    @staticmethod\n",
    "    def create_movie_ratings(ratings, movies):\n",
    "        return pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def load(ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "             ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "             movies_col_names=('item_id', 'title', 'genres'),\n",
    "             movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv'\n",
    "             ):\n",
    "        # Load movies\n",
    "        movies = MovieLensDataset.load_movies(movies_path=movies_path, movies_col_names=movies_col_names)\n",
    "        # Load ratings\n",
    "        ratings = MovieLensDataset.load_ratings(ratings_path=ratings_path, ratings_col_names=ratings_col_names)\n",
    "\n",
    "        # Merge the ratings and movies\n",
    "        movie_ratings = pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConstraint:\n",
    "    \"\"\"\n",
    "    TimeConstraint is a constraint on the timestamp of the movie ratings.\n",
    "    We classify a TimeConstraint as either max_time_constraint or time_bin_constraint.\n",
    "    max_time_constraint is used to simulate real life in which we do not know the future but all the data up until one point in time.\n",
    "    time_bin_constraint is used to grab a portion of a time interval where starting and ending points are strictly defined and data is well known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, end_dt, start_dt=None):\n",
    "        \"\"\"\n",
    "        When end_dt is only given, system will have a max time constraint only.\n",
    "\n",
    "        When end_dt and start_dt are given, system will have beginning end ending boundary.\n",
    "\n",
    "        :param end_dt: The maximum limit of the time constraint.\n",
    "        :param start_dt: The minimum limit of the time constraint.\n",
    "            Always set start_dt to None if you change the object from time_bin to max_limit.\n",
    "        \"\"\"\n",
    "        self.end_dt = end_dt\n",
    "        self.start_dt = start_dt\n",
    "\n",
    "    def is_valid_time_bin(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint object represents a valid time bin.\n",
    "        \"\"\"\n",
    "        if self.is_time_bin() and (self._end_dt > self._start_dt):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_valid_max_limit(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint represents a valid max time limit.\n",
    "        \"\"\"\n",
    "        if (self._end_dt is not None) and (self._start_dt is None):\n",
    "            return True\n",
    "\n",
    "    def is_time_bin(self) -> bool:\n",
    "        if (self._start_dt is not None) and (self._end_dt is not None):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Comparing TimeConstraints\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt == other.start_dt and self._end_dt == other.end_dt\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt != other.start_dt or self._end_dt != other.end_dt\n",
    "\n",
    "    # Properties\n",
    "\n",
    "    @property\n",
    "    def end_dt(self):\n",
    "        return self._end_dt\n",
    "\n",
    "    @end_dt.setter\n",
    "    def end_dt(self, value):\n",
    "        self._end_dt = value\n",
    "\n",
    "    @property\n",
    "    def start_dt(self):\n",
    "        return self._start_dt\n",
    "\n",
    "    @start_dt.setter\n",
    "    def start_dt(self, value):\n",
    "        self._start_dt = value\n",
    "\n",
    "    # Printing TimeConstraints\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"\"\"\n",
    "    Accuracy class provides utility methods in order to measure accuracy of our analysis.\n",
    "    \n",
    "    Supported Measures:\n",
    "    rmse, accuracy, balanced accuracy, informedness, markedness, \n",
    "    f1, mcc, precision, recall, specificity, NPV and \n",
    "    other threshold measures where we round ratings less than 3.5 to min rating, upper to max rating and use supported measures on this data.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rmse(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Root Mean Square Error of given list or Dataframe of (prediction, actual) data.\n",
    "        \n",
    "        In case rmse value is found 0, it is returned as 0.001 to differentiate between successfull rmse\n",
    "        calculation and erroneous calculations where number of predictions in data is zero.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In case dataframe of predictions wher each row[0]=prediction, row[1]=actual rating\n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                prediction = row[0]\n",
    "                # In case valid prediction is made(0 is invalid, minimum 0.5 in movielens dataset)\n",
    "                if prediction != 0:\n",
    "                    # Round the ratings to the closest half or exact number\n",
    "                    # since movielens dataset only containst ratings 0.5, 1, 1.5,..., 4, 4.5, 5\n",
    "                    actual = Accuracy.half_round_rating(row[1])\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "                if number_of_predictions == 0:\n",
    "                    return 0 \n",
    "                rmse_value = sum_of_square_differences / number_of_predictions\n",
    "            return rmse_value if rmse_value != 0 else 0.001\n",
    "        # In case list of predictions where each element is (prediction, actual)\n",
    "        elif type(predictions) is list:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:                  # if the prediction is valid\n",
    "                    actual = Accuracy.half_round_rating(actual)\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "            if number_of_predictions == 0:\n",
    "                return 0\n",
    "        \n",
    "            rmse_value = sum_of_square_differences / number_of_predictions \n",
    "            return rmse_value if rmse_value != 0 else 0.001    \n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_accuracy(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Threshold accuracy is the rate of sucessful prediction when we round \n",
    "        ratings between 0.5 and 3.5 to the lowest rating(0.5) ,\n",
    "        ratings between 3.5 and 5 to the highest rating(5)\n",
    "        \n",
    "        Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                # row[1] : actual rating, row[0] : prediction\n",
    "                prediction = row[0]\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(row[1])\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        elif type(predictions) is list:            \n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(actual)\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                        \n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \"\"\"\n",
    "        \n",
    "        TP, FN, FP, TN = Accuracy.threshold_confusion_matrix(predictions)\n",
    "        precision = Accuracy.precision(TP, FP)     # also called PPV\n",
    "        recall = Accuracy.recall(TP, FN)           # also called TPR\n",
    "        specificity = Accuracy.specificity(FP, TN) # also called TNR\n",
    "        NPV = Accuracy.negative_predictive_value(FN, TN)\n",
    "        \n",
    "        accuracy = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "        balanced_accuracy = Accuracy.balanced_accuracy(TPR=recall, TNR=specificity)\n",
    "        informedness = Accuracy.informedness(TPR=recall, TNR=specificity)\n",
    "        markedness = Accuracy.markedness(PPV=precision, NPV=NPV)\n",
    "        \n",
    "        f1 = Accuracy.f_measure(precision, recall)\n",
    "        mcc = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "                \n",
    "        output = {\n",
    "                  \"accuracy\"         :round(accuracy, 3),\n",
    "                  \"balanced_accuracy\":round(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :round(informedness, 3),\n",
    "                  \"markedness\"       :round(markedness, 3),\n",
    "                  \"f1\"               :round(f1, 3),\n",
    "                  \"mcc\"              :round(mcc, 3),\n",
    "                  \"precision\"        :round(precision, 3),\n",
    "                  \"recall\"           :round(recall, 3),\n",
    "                  \"specificity\"      :round(specificity, 3),\n",
    "                  \"NPV\"              :round(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \n",
    "        https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "        \n",
    "        Returns analysis for each class as list\n",
    "        :return: accuracy, balanced_accuracy, informedness, markedness, f1, mcc, precision, recall, specificity, NPV\n",
    "        \"\"\"\n",
    "        confusion_mtr = Accuracy.confusion_matrix(predictions)\n",
    "        \n",
    "        # Use macro averaging (https://stats.stackexchange.com/questions/187768/matthews-correlation-coefficient-with-multi-class)\n",
    "        precision = [0] * 10   # 10 is the number of classes found \n",
    "        recall = [0] * 10      # 0.5 -> Class 0 , 1 -> Class 1, 1.5 -> Class 2 .... \n",
    "        specificity = [0] * 10\n",
    "        NPV = [0] * 10 \n",
    "        \n",
    "        accuracy = [0] * 10 \n",
    "        balanced_accuracy = [0] * 10 \n",
    "        informedness = [0] * 10 \n",
    "        markedness = [0] * 10 \n",
    "        \n",
    "        f1 = [0] * 10 \n",
    "        mcc = [0] * 10\n",
    "        \n",
    "        for i in range(0, 10): # For Each Class\n",
    "            TP, FN, FP, TN = Accuracy.confusion_matrix_one_against_all(confusion_mtr, i)\n",
    "            precision[i] = Accuracy.precision(TP, FP)     # also called PPV\n",
    "            recall[i] = Accuracy.recall(TP, FN)           # also called TPR\n",
    "            specificity[i] = Accuracy.specificity(FP, TN) # also called TNR\n",
    "            NPV[i] = Accuracy.negative_predictive_value(FN, TN)\n",
    "\n",
    "            accuracy[i] = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "            balanced_accuracy[i] = Accuracy.balanced_accuracy(TPR=recall[i], TNR=specificity[i])\n",
    "            informedness[i] = Accuracy.informedness(TPR=recall[i], TNR=specificity[i])\n",
    "            markedness[i] = Accuracy.markedness(PPV=precision[i], NPV=NPV[i])\n",
    "\n",
    "            f1[i] = Accuracy.f_measure(precision[i], recall[i])\n",
    "            mcc[i] = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "        output = {\n",
    "                  \"accuracy\"         :Accuracy.round_list_elements(accuracy, 3),\n",
    "                  \"balanced_accuracy\":Accuracy.round_list_elements(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :Accuracy.round_list_elements(informedness, 3),\n",
    "                  \"markedness\"       :Accuracy.round_list_elements(markedness, 3),\n",
    "                  \"f1\"               :Accuracy.round_list_elements(f1, 3),\n",
    "                  \"mcc\"              :Accuracy.round_list_elements(mcc, 3),\n",
    "                  \"precision\"        :Accuracy.round_list_elements(precision, 3),\n",
    "                  \"recall\"           :Accuracy.round_list_elements(recall, 3),\n",
    "                  \"specificity\"      :Accuracy.round_list_elements(specificity, 3),\n",
    "                  \"NPV\"              :Accuracy.round_list_elements(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_list_elements(l, precision):\n",
    "        \"\"\"\n",
    "        :param l: list of floats\n",
    "        :param precision: precision after dot\n",
    "        \"\"\"\n",
    "        return [ round(x, precision) for x in l ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy_multi_class(confusion_mtr):\n",
    "        length = len(confusion_mtr)\n",
    "        numenator = 0\n",
    "        denuminator = 0 \n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                temp = confusion_mtr[i][j]\n",
    "                denuminator += temp\n",
    "                if i == j:\n",
    "                    numenator += temp\n",
    "        return numenator / denominator\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(TP, FN, FP, TN):\n",
    "        return  (TP + TN) / (TP + FN + FP + TN)\n",
    "    \n",
    "    @staticmethod\n",
    "    def balanced_accuracy(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return (TPR + TNR) / 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def informedness(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return TPR + TNR - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def markedness(PPV, NPV):\n",
    "        \"\"\"\n",
    "        :param PPV: Positive Predictive Value also known as precision\n",
    "        :param NPV: Negative Predictive Value\n",
    "        \"\"\"\n",
    "        return PPV + NPV - 1 \n",
    "    \n",
    "    @staticmethod\n",
    "    def precision(TP, FP):\n",
    "        \"\"\"\n",
    "        Also called as precision or positive predictive value (PPV)\n",
    "        \n",
    "        Precision = TP / (TP + FP) for binary class\n",
    "        Precision = TP / (All Predicted Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FP\n",
    "        return TP / denuminator if denuminator != 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_predictive_value(FN, TN):\n",
    "        \"\"\"     \n",
    "        NPV = TN / (TN + FN) for binary class\n",
    "        NPV = TN / (All Predicted Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TN + FN\n",
    "        return TN / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def recall(TP, FN):\n",
    "        \"\"\"\n",
    "        Also called as sensitivity, recall, hitrate, or true positive rate(TPR)\n",
    "        Recall = TP / (TP + FN) for binary class\n",
    "        Recall = TP / (All Actual Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FN\n",
    "        return TP / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def specificity(FP, TN):\n",
    "        \"\"\"\n",
    "        Also called as specificity, selectivity or true negative rate (TNR)\n",
    "        specificity = TN / (TP + FN) for binary class\n",
    "        specificity = TN / (All Actual Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = FP + TN \n",
    "        return TN / denuminator if denuminator != 0 else 0 \n",
    "    \n",
    "    @staticmethod\n",
    "    def f_measure(precision, recall):\n",
    "        \"\"\"\n",
    "        F-Measure is the harmonic mean of the precision and recall.\n",
    "        \"\"\"\n",
    "        sum_of_both = precision + recall\n",
    "        return (2 * precision * recall) / sum_of_both if sum_of_both != 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mcc(TP, FN, FP, TN):\n",
    "        \"\"\"\n",
    "        MCC(Matthews Correlation Coefficient)\n",
    "        \"\"\"\n",
    "        # Calulate Matthews Correlation Coefficient\n",
    "        numenator   = (TP * TN) - (FP * FN) \n",
    "        denominator = (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)\n",
    "        denominator = math.sqrt(denominator) if denominator > 0 else 0\n",
    "        return numenator / denominator if denominator != 0 else 0\n",
    "  \n",
    "# deprecated -> now, we use macro averaging instad of multi class versions of metrics\n",
    "#     @staticmethod\n",
    "#     def mcc_multi_class(confusion_mtr):\n",
    "#         # https://en.wikipedia.org/wiki/Matthews_correlation_coefficient#Multiclass_case\n",
    "#         # https://stats.stackexchange.com/questions/187768/matthews-correlation-coefficient-with-multi-class\n",
    "#         length = len(confusion_mtr)\n",
    "#         numenator = 0\n",
    "#         for k in range(length):\n",
    "#             for l in range(length):\n",
    "#                 for m in range(length):\n",
    "#                     numenator += confusion_mtr[k][k] * confusion_mtr[l][m]\n",
    "#                     numenator -= confusion_mtr[k][l] * confusion_mtr[m][k]\n",
    "\n",
    "#         denuminator_1 = 0\n",
    "#         for k in range(length):\n",
    "#             denuminator_part_1 = 0\n",
    "#             for l in range(length):\n",
    "#                 denuminator_part_1 += confusion_mtr[k][l]\n",
    "\n",
    "#             denuminator_part_2 = 0\n",
    "#             for f in range(length):\n",
    "#                 if f != k:\n",
    "#                     for g in range(length):\n",
    "#                         denuminator_part_2 += confusion_mtr[f][g]\n",
    "#             denuminator_1 += denuminator_part_1 * denuminator_part_2\n",
    "\n",
    "#         denuminator_2 = 0\n",
    "#         for k in range(length):\n",
    "#             denuminator2_part_1 = 0\n",
    "#             for l in range(length):\n",
    "#                 denuminator2_part_1 += confusion_mtr[l][k]\n",
    "\n",
    "#             denuminator2_part_2 = 0\n",
    "#             for f in range(length):\n",
    "#                 if f != k:\n",
    "#                     for g in range(length):\n",
    "#                         denuminator2_part_2 += confusion_mtr[g][f]\n",
    "#             denuminator_2 += denuminator2_part_1 * denuminator2_part_2\n",
    "#         return numenator / math.sqrt(denuminator_1) * math.sqrt(denuminator_2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusion_matrix_one_against_all(confusion_mtr, class_i):\n",
    "        \"\"\"\n",
    "        Create binary confusion matrix out of multi-class confusion matrix\n",
    "        \n",
    "        Positive Class: class_i\n",
    "        Negative Class: non class_i\n",
    "                \n",
    "        TP: True Positive   FN: False Negative\n",
    "        FP: False Positive  TN: True Negative\n",
    "        \n",
    "        \"TP of Class_1\" is all Class_1 instances that are classified as Class_1.\n",
    "        \"TN of Class_1\" is all non-Class_1 instances that are not classified as Class_1.\n",
    "        \"FP of Class_1\" is all non-Class_1 instances that are classified as Class_1.\n",
    "        \"FN of Class_1\" is all Class_1 instances that are not classified as Class_1.\n",
    "        # https://www.researchgate.net/post/How_do_you_measure_specificity_and_sensitivity_in_a_multiple_class_classification_problem\n",
    "        \n",
    "        --> Input matrix\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        \n",
    "        --> Output matrix\n",
    "        \n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "        \n",
    "        :param confusion_mtr: 10 class confusion matrix designed for movielens\n",
    "        :param class_i: index of the class we are interested in(0-9)\n",
    "        :return: TP, FN, FP, TN\n",
    "        \"\"\"\n",
    "        length = len(confusion_mtr)\n",
    "\n",
    "        TP = confusion_mtr[class_i][class_i] \n",
    "        \n",
    "        actual_class_i_count = 0 \n",
    "        for i in range(length):  # sum of the row\n",
    "            actual_class_i_count += confusion_mtr[class_i][i]\n",
    "        FN = actual_class_i_count - TP\n",
    "        \n",
    "        predicted_class_i_count = 0\n",
    "        for i in range(length): # sum of the column\n",
    "            predicted_class_i_count += confusion_mtr[i][class_i]\n",
    "        FP = predicted_class_i_count - TP\n",
    "        \n",
    "        # sum of matrix\n",
    "        sum_of_matrix = np.sum(confusion_mtr)\n",
    "        # TN is found by summing up all values except the row and column of the class \n",
    "        TN = sum_of_matrix - predicted_class_i_count - actual_class_i_count - TP \n",
    "        \n",
    "        return TP, FN, FP, TN\n",
    "        \n",
    "    @staticmethod\n",
    "    def confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        0 Class: 0.5\n",
    "        1 Class: 1\n",
    "        2 Class: 1.5\n",
    "        3 Class: 2\n",
    "        4 Class: 2.5\n",
    "        5 Class: 3\n",
    "        6 Class: 3.5\n",
    "        7 Class: 4\n",
    "        8 Class: 4.5\n",
    "        9 Class: 5\n",
    "\n",
    "        T0: True 0\n",
    "        F0: False 0\n",
    "        T1: True 1\n",
    "        F1: False 1\n",
    "        ...\n",
    "\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        ...\n",
    "        \"\"\"\n",
    "        # Create multiclass confusion matrix\n",
    "\n",
    "        conf_mtr = np.zeros( (10,10) )\n",
    "\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.half_round_rating(prediction[0])\n",
    "            actual    = Accuracy.half_round_rating(prediction[1])\n",
    "\n",
    "            predicted_class_index = int( (predicted * 2) - 1 )\n",
    "            actual_class_index = int( (actual * 2) - 1 )\n",
    "\n",
    "            conf_mtr[actual_class_index][predicted_class_index] += 1\n",
    "\n",
    "        return conf_mtr\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        Positive Class: 5\n",
    "        Negative Class: 0.5\n",
    "\n",
    "        TP: True Positive\n",
    "        TN: True Negative\n",
    "        FP: False Positive\n",
    "        FN: False Negative\n",
    "\n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "\n",
    "        \"\"\"\n",
    "        # Create confusion matrix\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.threshold_round_rating(prediction[0])\n",
    "            actual = Accuracy.threshold_round_rating(prediction[1])\n",
    "            if predicted == 5 and actual == 5:\n",
    "                TP += 1\n",
    "            elif predicted == 5 and actual == 0.5:\n",
    "                FP += 1\n",
    "            elif predicted == 0.5 and actual == 0.5:\n",
    "                TN += 1\n",
    "            elif predicted == 0.5 and actual == 5:\n",
    "                FN += 1\n",
    "        return TP, FN, FP, TN\n",
    "\n",
    "    @staticmethod\n",
    "    def half_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in the movielens dataset\n",
    "        For ex.\n",
    "          ratings between 2 and 2.25 -> round to 2\n",
    "          ratings between 2.25 and 2.5 -> round to 2.5\n",
    "          ratings between 2.5 and 2.75 -> round to 2.5\n",
    "          ratings between 2.75 and 3 -> round to 3\n",
    "\n",
    "        \"\"\"\n",
    "        floor_value = math.floor(rating)\n",
    "        if(rating > floor_value + 0.75):\n",
    "            return floor_value + 1\n",
    "        elif(rating > floor_value + 0.5 or rating > floor_value + 0.25):\n",
    "            return floor_value + 0.5\n",
    "        else:\n",
    "            return floor_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in threshold fashion\n",
    "          ratings between 0.5 and 3.5 -> round to 0.5\n",
    "          ratings between 3.5 and 5 -> round to 5\n",
    "        \"\"\"\n",
    "        if (0.5 <= rating < 3.5):\n",
    "            return 0.5\n",
    "        elif (3.5 <= rating <= 5):\n",
    "            return 5\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainsetUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainsetUser:\n",
    "    \"\"\"\n",
    "    TrainsetUser provides user related dataset utility methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset:Dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if not self.dataset.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "    \n",
    "    def get_users(self):\n",
    "        \"\"\"\n",
    "        Get list of unique 'user_id's\n",
    "        \"\"\"\n",
    "        if self.dataset.is_ratings_cached:\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "        \n",
    "        return pd.unique(data['user_id'])\n",
    "    \n",
    "    def get_active_users(self, n=10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Users in sorted order where the first one is the one who has given most ratings.\n",
    "\n",
    "        :param n: Number of users to retrieve.\n",
    "        :return: user DataFrame with index of 'user_id' and columns of ['mean_rating', 'No_of_ratings'] .\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataset.is_ratings_cached:                         # 30% faster than other choice\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "\n",
    "        active_users = pd.DataFrame(data.groupby('user_id')['rating'].mean())\n",
    "        active_users['No_of_ratings'] = pd.DataFrame(data.groupby('user_id')['rating'].count())\n",
    "        active_users.sort_values(by=['No_of_ratings'], ascending=False, inplace=True)\n",
    "        active_users.columns = ['mean_rating', 'No_of_ratings']\n",
    "        return active_users.head(n)\n",
    "    \n",
    "    def get_random_users(self, n=1):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'user_id's\n",
    "\n",
    "        :param n: Number of random users\n",
    "        :return: List of random 'user_id's\n",
    "        \"\"\"\n",
    "\n",
    "        return random.choices(population=self.get_users(), k=n)\n",
    "    \n",
    "    def get_user_ratings(self, user_id: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the ratings given by of the chosen users\n",
    "\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Ratings given by the 'user_id'\n",
    "        \"\"\"\n",
    "        if self.dataset.is_ratings_cached:                         # 2.2x faster than other choice\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "\n",
    "        return data.loc[data['user_id'] == user_id][['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "    \n",
    "    def get_user_ratings_at(self, user_id: int, time_constraint:TimeConstraint=None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get user ratings up until the given datetime\n",
    "        :param user_id: id of the chosen user\n",
    "        :param time_constraint: type of the time constraint.\n",
    "        :return: Ratings given by the 'user_id' before given datetime\n",
    "        \"\"\"\n",
    "        # If no time constraint, thes apply normal get_user_ratings method\n",
    "        if time_constraint is None:\n",
    "            return self.get_user_ratings(user_id)\n",
    "        \n",
    "        if self.dataset.is_ratings_cached:\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "         \n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            result = data.loc[(data['user_id'] == user_id) & (data['timestamp'] < time_constraint.end_dt)]\n",
    "        \n",
    "        elif time_constraint.is_valid_time_bin():\n",
    "            result = data.loc[(data['user_id'] == user_id)\n",
    "                            & (data.timestamp >= time_constraint.start_dt)\n",
    "                            & (data.timestamp < time_constraint.end_dt)]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return result[['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "    \n",
    "    \n",
    "    def get_random_rating(self, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get random rating of the given user.\n",
    "\n",
    "        :param user_id: User of interest\n",
    "        :return:  movie_id or item_id of the random rating given by the user.\n",
    "                  In case non-valid user_id supplied then returns 0\n",
    "        \"\"\"\n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return random.choice(user_ratings.index.values.tolist()) if not user_ratings.empty else 0\n",
    "    \n",
    "    def get_random_ratings(self, user_id: int, n=2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get random n ratings given by the user. Only use when n > 2\n",
    "\n",
    "        Use get_random_rating if n=1 since that one 2 fold faster.\n",
    "\n",
    "        :param user_id: the user of interest\n",
    "        :param n: number of random movies to get\n",
    "        :return: DataFrame of movies, if no item found then empty DataFrame\n",
    "        \"\"\"\n",
    "        if n == 1:\n",
    "            return get_random_rating(user_id)\n",
    "            \n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return random.choices(population=user_ratings.index.values.tolist(),\n",
    "                              k=n) if not user_ratings.empty else user_ratings\n",
    "    \n",
    "    def get_random_rating_per_user(self, user_id_list):\n",
    "        \"\"\"\n",
    "        Get random movie rating for each user given in the 'user_id_list'\n",
    "\n",
    "        :param user_id_list: List of valid user_ids\n",
    "        :return: List of (user_id, movie_id) tuples\n",
    "                where each movie_id is randomly chosen from given ratings of the user_id .\n",
    "                In case any one of the user_id's supplies invalid, then the movie_id will be 0 for that user.\n",
    "        \"\"\"        \n",
    "\n",
    "    def get_random_movie_per_user(self, user_id_list):\n",
    "        \"\"\"\n",
    "        Get random movie for each user given in the 'user_id_list'\n",
    "\n",
    "        :param user_id_list: List of valid user_ids\n",
    "        :return: List of (user_id, movie_id) tuples\n",
    "                where each movie_id is randomly chosen from watched movies of the user_id .\n",
    "                In case any one of the user_id's supplies invalid, then the movie_id will be 0 for that user.\n",
    "        \"\"\"\n",
    "        user_movie_list = list()\n",
    "        for user_id in user_id_list:\n",
    "            user_movie_list.append((user_id, self.get_random_rating(user_id)))\n",
    "        return user_movie_list\n",
    "    \n",
    "    def get_user_avg(self, user_id: int):\n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "\n",
    "    def get_user_avg_at(self, user_id: int, at: datetime):\n",
    "        user_ratings = self.get_user_ratings_at(user_id, at)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "    \n",
    "    def get_user_avg_timestamp(self, user_id: int):\n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return user_ratings.timestamp.mean() if not user_ratings.empty else 0\n",
    "    \n",
    "    def get_first_timestamp(self):\n",
    "        if self.dataset.is_ratings_cached:\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "        return data['timestamp'].min()\n",
    "    \n",
    "    def get_last_timestamp(self):\n",
    "        if self.dataset.is_ratings_cached:\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "        return data['timestamp'].max()\n",
    "    \n",
    "    def get_timestamp(self, user_id: int, movie_id: int):\n",
    "        \"\"\"\n",
    "        Get the timestamp of the given rating\n",
    "\n",
    "        :param user_id: the users whose rating timestamp we are searching\n",
    "        :param movie_id: id of the movie that the user gave the rating\n",
    "        :return: if found the datetime object otherwise None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataset.is_ratings_cached:\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "\n",
    "        timestamp = data.loc[(data['user_id'] == user_id) & (data['item_id'] == movie_id)]\n",
    "        return timestamp.values[0, 3] if not timestamp.empty else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainsetMovie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainsetMovie:\n",
    "    \"\"\"\n",
    "    TrainsetMovie provides movie related dataset utility methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset:Dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if not self.dataset.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "    \n",
    "    def get_movie(self, movie_id:int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Movie Record\n",
    "\n",
    "        :return: DataFrame which contains the given 'movie_id's details. If not found empty DataFrame .\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.dataset.is_movies_cached:\n",
    "            data = self.dataset.movies\n",
    "            return data.iloc[movie_id - 1]\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "            return data.loc[data['item_id'] == movie_id]\n",
    "    \n",
    "    def get_movies(self):\n",
    "        \"\"\"\n",
    "        Get list of unique 'item_id's or in other words the movies.\n",
    "\n",
    "        :return: List of movie ids\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataset.is_movies_cached:\n",
    "            return self.dataset.movies.index.values.tolist()\n",
    "\n",
    "        return pd.unique(self.dataset.movie_ratings['item_id']).tolist()\n",
    "    \n",
    "    def get_random_movies(self, n=10):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'item_id's or in other words the movies\n",
    "\n",
    "        :param n: Number of random movies\n",
    "        :return: List of random 'movie_id's\n",
    "        \"\"\"\n",
    "        return random.choices(population=self.get_movies(), k=n)\n",
    "    \n",
    "    def get_movie_rating(self, movie_id: int, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the movie rating taken by the chosen user\n",
    "\n",
    "        :param movie_id: the movie chosen movie's id\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Rating given by user. If not found, returns 0\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataset.is_ratings_cached:\n",
    "            data = self.dataset.ratings\n",
    "        else:\n",
    "            data = self.dataset.movie_ratings\n",
    "\n",
    "        movie_rating = data.loc[(data['user_id'] == user_id) & (data['item_id'] == movie_id)]\n",
    "        return movie_rating.values[0, 2] if not movie_rating.empty else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pearson:\n",
    "    \"\"\"\n",
    "    Pearson is the classic way of handling recommendations. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset:Dataset, min_common_elements: int=5, \n",
    "                 cache_user_corrs=True, corr_time_constraint:TimeConstraint=None):\n",
    "        self.dataset = dataset\n",
    "        self.min_common_elements = min_common_elements\n",
    "        self.trainset_user = TrainsetUser(dataset)\n",
    "        self.trainset_movie = TrainsetMovie(dataset)\n",
    "        \n",
    "        # Caching User Correlations\n",
    "        self.cache_user_corrs = cache_user_corrs\n",
    "        self.user_corrs = None\n",
    "        self.time_constraint = corr_time_constraint\n",
    "        if cache_user_corrs is True:\n",
    "            self.user_corrs = Pearson.create_user_corrs(dataset.movie_ratings, corr_time_constraint, min_common_elements)\n",
    "    \n",
    "    \n",
    "    def get_corr_matrix(self):\n",
    "        \"\"\"\n",
    "        If user corrs are cached, returns the cache\n",
    "        Otherwise creates new user corrs using this class parameters and caches it if caching open for future use.\n",
    "        \"\"\"\n",
    "        \n",
    "        ## No caching exists, then create new correlation matrix\n",
    "        if self.cache_user_corrs is False:\n",
    "            return Pearson.create_user_corrs(self.dataset.movie_ratings, self.time_constraint, self.min_common_elements)\n",
    "        \n",
    "        ## If Caching exists\n",
    "        \n",
    "        # Check whether cache hit or not\n",
    "        if self.user_corrs is not None:\n",
    "            return self.user_corrs\n",
    "        \n",
    "        # In case cache is being invalided by someone, create new user corrs and cache it before returning.\n",
    "        \n",
    "        self.user_corrs = Pearson.create_user_corrs(self.dataset.movie_ratings, self.time_constraint, self.min_common_elements)\n",
    "        return self.user_corrs\n",
    "    \n",
    "    def get_user_corrs(self, user_id):\n",
    "        \"\"\"\n",
    "        The the correlations of the user of interest to the other users.  \n",
    "        \"\"\"\n",
    "        # Get the chosen 'user_id's correlations\n",
    "        user_corr_matrix = self.get_corr_matrix()\n",
    "        user_correlations = user_corr_matrix.get(user_id)\n",
    "        if user_correlations is None:\n",
    "            return None\n",
    "        \n",
    "        # Drop any null, if found\n",
    "        user_correlations.dropna(inplace=True)\n",
    "        \n",
    "        # Create A DataFrame from not-null correlations of the 'user_id'\n",
    "        users_alike = pd.DataFrame(user_correlations)\n",
    "        \n",
    "        # Rename the only column to 'corr'\n",
    "        users_alike.columns = ['corr']\n",
    "        \n",
    "        # Sort the user correlations in descending order\n",
    "        #     so that first one is the most similar, last one least similar\n",
    "        users_alike.sort_values(by='corr', ascending=False, inplace=True)\n",
    "        \n",
    "        # Eliminate Correlation to itself by deleting first row,\n",
    "        #     since biggest corr is with itself it is in first row\n",
    "        return users_alike.iloc[1:]\n",
    "    \n",
    "    def get_k_nearest_neighbours(self, user_id, k=10, time_constraint=None):\n",
    "        \"\"\"\n",
    "        :param user_id: the user of interest\n",
    "        :param k: number of neighbours to retrieve\n",
    "        :param time_constraint: time constraint when choosing neighbours\n",
    "        :return: Returns the k neighbours and correlations in between them. If no neighbours found, returns None\n",
    "                 DataFrame which has 'Correlation' column and 'user_id' index.\n",
    "        \"\"\"\n",
    "        # Invalidate cache if not the same time constraint\n",
    "        if self.time_constraint != time_constraint:\n",
    "            self.user_corrs = None\n",
    "            self.time_constraint = time_constraint  # Update the time constraint for future calculations\n",
    "        \n",
    "        # Get The Correlations with The Other Users\n",
    "        user_correlations = self.get_user_corrs(user_id)\n",
    "        \n",
    "        if user_correlations is None:\n",
    "            return None\n",
    "        \n",
    "        return user_correlations.head(k)\n",
    "    \n",
    "    def pearson(self, first_user_id, second_user_id, time_constraint:TimeConstraint=None):\n",
    "        \"\"\"\n",
    "        Calculate pearson correlation in between two users.\n",
    "        \n",
    "        If more than 1 user correlation will be calculated \n",
    "        using the same time_constraint, then use the create_user_corrs method.\n",
    "        \"\"\"\n",
    "        x = self.trainset_user.get_user_ratings_at(first_user_id, time_constraint)\n",
    "        y = self.trainset_user.get_user_ratings_at(second_user_id, time_constraint)\n",
    "        \n",
    "        merged = x.merge(y, on='item_id')\n",
    "        if merged.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Pearson Correlation in between the timebin x and y\n",
    "        x_avg = self.trainset_user.get_user_avg(first_user_id)\n",
    "        y_avg = self.trainset_user.get_user_avg(second_user_id)\n",
    "        \n",
    "        numenator    = ((merged['rating_x'] - x_avg) * (merged['rating_y'] - y_avg)).sum()\n",
    "        denominator  = math.sqrt(((merged['rating_x'] - x_avg) ** 2).sum())\n",
    "        denominator *= math.sqrt(((merged['rating_y'] - y_avg) ** 2).sum())\n",
    "\n",
    "        return numenator / denominator\n",
    "\n",
    "    def predict(self, user_id, movie_id, k=10, time_constraint:TimeConstraint=None) -> float:\n",
    "        \"\"\"\n",
    "        Calculate prediction by using weighted average\n",
    "\n",
    "        :param user_id: user of interest\n",
    "        :param movie_id: the movie's rating is the one we we want to predict\n",
    "        :param k: number of neighbours to retrieve\n",
    "        :param time_constraint: time constraint when choosing neighbours\n",
    "        :return: Prediction rating\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Get The K Neareast Neighbours\n",
    "        k_neighbours = self.get_k_nearest_neighbours(user_id, k, time_constraint)\n",
    "        \n",
    "        \n",
    "        # If a movie with movie_id not exists, predict 0\n",
    "        if self.trainset_movie.get_movie(movie_id).empty:\n",
    "            return 0\n",
    "        \n",
    "        if k_neighbours is None or k_neighbours.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate weighted average by normalizing to the 0\n",
    "        user_avg_rating = self.trainset_user.get_user_avg(user_id)\n",
    "        \n",
    "        weighted_sum = 0.0                          # numerator of the weighted average\n",
    "        sum_of_weights = 0.0                        # denominator of the weighted average\n",
    "        \n",
    "        for neighbour_id, data in k_neighbours.iterrows():\n",
    "            # Get each neighbour's correlation and her/his rating to 'movie_id'\n",
    "            neighbour_corr = data['corr']\n",
    "            neighbour_rating = self.trainset_movie.get_movie_rating(movie_id=movie_id, user_id=neighbour_id)\n",
    "            # If the neighbour hasn't given any rating to the movie_id\n",
    "            if neighbour_rating == 0:   \n",
    "                continue                     # pass this around of the loop\n",
    "            neighbour_avg_rating = self.trainset_user.get_user_avg(user_id=neighbour_id)\n",
    "            \n",
    "            neighbour_zero_centered_rating = neighbour_rating - neighbour_avg_rating\n",
    "            # Calculate Weighted sum and sum of weights\n",
    "            weighted_sum += neighbour_zero_centered_rating * neighbour_corr\n",
    "            sum_of_weights += neighbour_corr\n",
    "        # Predict\n",
    "        if sum_of_weights != 0:\n",
    "            prediction_rating = user_avg_rating + (weighted_sum / sum_of_weights)\n",
    "        else:\n",
    "            prediction_rating = 0  # In this case, none of the neighbours have given rating to 'the movie'\n",
    "\n",
    "        return prediction_rating if prediction_rating <= 5 else 5       \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_user_corrs(movie_ratings, time_constraint: TimeConstraint=None, min_common_elements=5):\n",
    "        # by default movie_ratings is for no time constraint\n",
    "        # with these controls, change the time constraint of the movie_ratings\n",
    "        if time_constraint is not None:\n",
    "            if time_constraint.is_valid_max_limit():\n",
    "                movie_ratings = movie_ratings[movie_ratings.timestamp < time_constraint.end_dt]\n",
    "            elif time_constraint.is_valid_time_bin():\n",
    "                movie_ratings = movie_ratings[(movie_ratings.timestamp >= time_constraint.start_dt)\n",
    "                                              & (movie_ratings.timestamp < time_constraint.end_dt)]\n",
    "\n",
    "        user_movie_matrix = movie_ratings.pivot_table(index='title', columns='user_id', values='rating')\n",
    "        return user_movie_matrix.corr(method=\"pearson\", min_periods=min_common_elements)\n",
    "    \n",
    "    @property\n",
    "    def time_constraint(self):\n",
    "        return self._time_constraint\n",
    "\n",
    "    @time_constraint.setter\n",
    "    def time_constraint(self, value):\n",
    "        self._time_constraint = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainset:\n",
    "    \"\"\"\n",
    "    Trainset class is used to predict movies using Pearson class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset:Dataset, min_common_elements:int=5, time_constraint:TimeConstraint=None):\n",
    "        \n",
    "        if not dataset.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.min_common_elements = min_common_elements\n",
    "        self.time_constraint = time_constraint\n",
    "        self.similarity = Pearson(dataset, min_common_elements=min_common_elements\n",
    "                                  ,cache_user_corrs=True, corr_time_constraint=time_constraint)\n",
    "        \n",
    "        self.trainset_user = TrainsetUser(dataset)\n",
    "        self.trainset_movie = TrainsetMovie(dataset)\n",
    "    \n",
    "    def predict_movies_watched(self, user_id, n=10, k=10, time_constraint=None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        :param user_id: user of interest\n",
    "        :param n: Number of movies to predict\n",
    "        :param k: k neighbours to take into account\n",
    "        :param time_constraint: When calculating k neighbours,\n",
    "                                only those that comply to time_constraints will be taken into account.\n",
    "        :return: DataFrame of Predictions where columns = ['prediction', 'rating'] index = 'movie_id'\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Get all movies watched by a user\n",
    "        movies_watched = self.trainset_user.get_user_ratings(user_id)\n",
    "        \n",
    "        if movies_watched.empty:\n",
    "            return None\n",
    "        \n",
    "        predictions = list()\n",
    "        number_of_predictions = 0\n",
    "        \n",
    "        for movie_id, data in movies_watched.iterrows():\n",
    "            if number_of_predictions == n:\n",
    "                break\n",
    "            \n",
    "            actual_rating = data['rating']\n",
    "            #timestamp    = data['timestamp']\n",
    "            \n",
    "            prediction = self.similarity.predict(user_id=user_id,movie_id=movie_id,\n",
    "                                                 k=k, time_constraint=time_constraint)\n",
    "            predictions.append( [prediction, actual_rating, movie_id] )\n",
    "            number_of_predictions += 1\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions, columns=['prediction', 'rating', 'movie_id'])\n",
    "        predictions_df.movie_id = predictions_df.movie_id.astype(int)\n",
    "        return predictions_df.set_index('movie_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimebinSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NetflixDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1227322</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2004-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>525356</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2004-07-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1927580</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2004-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>716874</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>883478</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058189</th>\n",
       "      <td>387418</td>\n",
       "      <td>4499</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2004-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058196</th>\n",
       "      <td>1114324</td>\n",
       "      <td>4499</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2005-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058204</th>\n",
       "      <td>1932594</td>\n",
       "      <td>4499</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2005-02-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058237</th>\n",
       "      <td>811530</td>\n",
       "      <td>4499</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2004-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058244</th>\n",
       "      <td>1852040</td>\n",
       "      <td>4499</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2004-05-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484806 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  item_id  rating  timestamp\n",
       "26        1227322        1     4.0 2004-02-06\n",
       "37         525356        1     2.0 2004-07-11\n",
       "61        1927580        1     4.0 2004-11-08\n",
       "62         716874        1     5.0 2005-05-06\n",
       "70         883478        1     4.0 2005-12-16\n",
       "...           ...      ...     ...        ...\n",
       "24058189   387418     4499     2.0 2004-02-10\n",
       "24058196  1114324     4499     1.0 2005-10-13\n",
       "24058204  1932594     4499     1.0 2005-02-15\n",
       "24058237   811530     4499     4.0 2004-07-28\n",
       "24058244  1852040     4499     1.0 2004-05-13\n",
       "\n",
       "[484806 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.ratings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
